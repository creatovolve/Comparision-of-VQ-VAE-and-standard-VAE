{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a6bab73-5226-4b2d-a457-5459d66fb853",
   "metadata": {},
   "source": [
    "# üß† VAE vs. VQ-VAE on CIFAR-10  \n",
    "### A Comparative Study in Latent Representation Learning\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Objective\n",
    "\n",
    "The goal of this project is to **train and compare** two generative autoencoding models ‚Äî  \n",
    "a **standard Variational Autoencoder (VAE)** and a **Vector Quantized Variational Autoencoder (VQ-VAE)** ‚Äî  \n",
    "using the **CIFAR-10** image dataset.  \n",
    "\n",
    "Both models aim to **learn compact latent representations** of natural images,  \n",
    "but they differ fundamentally in how the latent space is structured and optimized:\n",
    "\n",
    "- **VAE** uses a *continuous* latent space, optimized via the reparameterization trick and a KL-divergence regularization term.  \n",
    "- **VQ-VAE** introduces a *discrete* latent space using **vector quantization**, enabling the model to represent images through a finite learned codebook.\n",
    "\n",
    "By the end of this notebook, we will:\n",
    "1. Train both models on CIFAR-10 images.  \n",
    "2. Visualize reconstructions and generated samples.  \n",
    "3. Analyze qualitative differences between continuous and discrete latent representations.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Why Compare VAE and VQ-VAE?\n",
    "\n",
    "- **VAE** is a classical approach that provides smooth latent interpolation but often suffers from blurry reconstructions.  \n",
    "- **VQ-VAE**, on the other hand, replaces the continuous latent space with a learned dictionary of embeddings, which can lead to sharper image reconstructions and more interpretable discrete codes.\n",
    "\n",
    "This comparison helps demonstrate how quantization can influence both *image quality* and *latent structure*,  \n",
    "providing insights that are directly relevant to downstream tasks like **compression**, **generation**, and **representation learning**.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Dataset: CIFAR-10\n",
    "\n",
    "- **Dataset**: CIFAR-10 (60,000 color images, 10 classes, 32√ó32 pixels)  \n",
    "- **Training set**: 50,000 images  \n",
    "- **Test set**: 10,000 images  \n",
    "- **Normalization**: Pixel values scaled to [0, 1]  \n",
    "\n",
    "CIFAR-10 is a balanced, compact dataset commonly used to benchmark generative models, making it ideal for comparing different VAE variants under similar training conditions.\n",
    "\n",
    "---\n",
    "\n",
    "## üß∞ Frameworks Used\n",
    "\n",
    "This notebook is implemented with **PyTorch** and **TorchVision**, leveraging:\n",
    "- `torch.nn` for defining deep neural networks  \n",
    "- `torchvision.datasets` for loading CIFAR-10  \n",
    "- `torch.utils.data.DataLoader` for efficient batching  \n",
    "- `torchvision.utils` and `matplotlib` for visualization  \n",
    "- `tqdm` for progress tracking during training  \n",
    "\n",
    "---\n",
    "\n",
    "## üìà Expected Outcomes\n",
    "\n",
    "By the end of this notebook, you will obtain:\n",
    "- A trained **VAE** and **VQ-VAE** model on CIFAR-10  \n",
    "- Visual comparisons of **reconstructed** and **sampled** images  \n",
    "- A clear understanding of how quantization affects generative performance  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cb8ae1-25bc-4243-880e-ec723ba20cc2",
   "metadata": {},
   "source": [
    "## üß© Environment Setup and Imports\n",
    "\n",
    "Before building and training our models, we begin by importing all the necessary Python libraries and modules.\n",
    "\n",
    "- **Core Libraries:**  \n",
    "  - `os`, `math`, `random`, and `pathlib` provide essential utilities for file management, mathematical operations, and reproducibility.  \n",
    "  - `tqdm` adds progress bars for cleaner, real-time feedback during training.  \n",
    "  - `numpy` and `matplotlib` are used for numerical computations and data visualization.  \n",
    "\n",
    "- **PyTorch Framework:**  \n",
    "  - `torch` and `torch.nn` form the foundation for defining and training deep learning models.  \n",
    "  - `torch.nn.functional` provides functional interfaces for activation functions and loss computations.  \n",
    "  - `torch.utils.data.DataLoader` handles batching and shuffling of training data efficiently.  \n",
    "\n",
    "- **TorchVision Utilities:**  \n",
    "  - `torchvision` and `torchvision.transforms` are used to load and preprocess the **CIFAR-10 dataset**.  \n",
    "  - `torchvision.utils` provides convenient functions such as `make_grid` and `save_image` to visualize model outputs during and after training.\n",
    "\n",
    "The `%matplotlib inline` magic command ensures that all plots generated using `matplotlib` are displayed directly within the notebook cells.\n",
    "\n",
    "This setup provides a clean, modular foundation for developing and comparing our **VAE** and **VQ-VAE** models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d741b-26f7-4384-bb60-23fcd22756c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid, save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7450a1f0-85cd-4850-93f5-8d65254d8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ‚öôÔ∏è Hardware Setup, Reproducibility, and Precision Control\n",
    "\n",
    "Before training our models, we configure key runtime settings to ensure consistent results and optimal GPU utilization.\n",
    "\n",
    "- **Mixed Precision Training:**  \n",
    "  We import `GradScaler` and `autocast` from `torch.cuda.amp`, which enable **automatic mixed precision (AMP)**.  \n",
    "  Mixed precision allows certain parts of the computation to run in lower precision (float16) while keeping model stability in higher precision (float32), leading to faster training and reduced GPU memory usage ‚Äî especially useful on GPUs like the **NVIDIA P100**.\n",
    "\n",
    "- **Reproducibility:**  \n",
    "  To make results consistent across runs, we fix random seeds for Python, NumPy, and PyTorch using the value `42`.  \n",
    "  This ensures that data shuffling, weight initialization, and other stochastic processes behave deterministically.\n",
    "\n",
    "- **Device Configuration:**  \n",
    "  The training automatically uses **GPU (CUDA)** if available; otherwise, it falls back to CPU.  \n",
    "  Using GPU acceleration is crucial for deep generative models like VAE and VQ-VAE, as it significantly speeds up both forward and backward passes.\n",
    "\n",
    "- **Output Directory:**  \n",
    "  Finally, an output directory named `./outputs` is created to store generated images, checkpoints, and training logs.  \n",
    "  This helps maintain a clean and organized project structure throughout the experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270f0161-4ebc-42c6-b954-ed45c7a47a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed precision scaler\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch import amp\n",
    "\n",
    "autocast = amp.autocast\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# Output directory\n",
    "OUTDIR = Path(\"./outputs\")\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be45300a-7a62-4e89-932e-8899a03241ca",
   "metadata": {},
   "source": [
    "## üéõÔ∏è Hyperparameter Configuration\n",
    "\n",
    "This section defines the core hyperparameters that govern model training, optimization, and experiment behavior.  \n",
    "Each setting is tuned for training **VAE** and **VQ-VAE** on **CIFAR-10** with efficient GPU utilization (e.g., NVIDIA P100).\n",
    "\n",
    "- **EPOCHS:**  \n",
    "  Controls the total number of full passes through the dataset.  \n",
    "  Higher values (150‚Äì300) yield better convergence and reconstruction quality, though training time increases accordingly.\n",
    "\n",
    "- **BATCH_SIZE:**  \n",
    "  Determines how many samples are processed per GPU iteration.  \n",
    "  A batch size of `128` provides a good balance between GPU memory usage and stable gradient estimation on a 16GB P100.\n",
    "\n",
    "- **LR (Learning Rate):**  \n",
    "  The step size used by the optimizer.  \n",
    "  A rate of `2e-4` is a common, stable choice for training VAEs and VQ-VAEs with Adam or AdamW optimizers.\n",
    "\n",
    "- **VAE_ZDIM:**  \n",
    "  Dimensionality of the latent space in the **standard VAE**.  \n",
    "  This controls the capacity of the model to encode image features.\n",
    "\n",
    "- **VQ_EMBED_DIM** and **VQ_N_EMBED:**  \n",
    "  Define the embedding dimensionality and vocabulary size of the **VQ-VAE codebook**.  \n",
    "  These parameters directly affect the compression level and the representational diversity of the learned discrete latent space.\n",
    "\n",
    "- **BETA:**  \n",
    "  The KL-divergence weight in the VAE loss function.  \n",
    "  Adjusting `BETA` changes the trade-off between reconstruction fidelity and latent regularization.\n",
    "\n",
    "- **NUM_SAMPLES:**  \n",
    "  Number of generated images to sample at the end of training for qualitative comparison between models.\n",
    "\n",
    "- **SAVE_EVERY:**  \n",
    "  Specifies how frequently (in epochs) model checkpoints and image grids are saved to disk.\n",
    "\n",
    "- **USE_AMP:**  \n",
    "  Enables **Automatic Mixed Precision (AMP)** training for faster computation and reduced GPU memory load.\n",
    "\n",
    "> üí° *Tuning these parameters allows you to trade off between training speed, memory efficiency, and model quality.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f801517-a6b8-464c-876a-ea2fe3a25644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For \"full quality\" on a P100: epochs ~ 150-300 recommended; set below and adjust to your run-time budget.\n",
    "EPOCHS = 200                  # set to 200 for full-quality; reduce for quicker runs\n",
    "BATCH_SIZE = 128              # P100 16GB: 128 should be fine; reduce if OOM\n",
    "LR = 2e-4\n",
    "VAE_ZDIM = 128\n",
    "VQ_EMBED_DIM = 64\n",
    "VQ_N_EMBED = 512\n",
    "BETA = 1.0                    # VAE KL weight\n",
    "NUM_SAMPLES = 40              # number of samples to generate at the end (30-40 as you requested)\n",
    "SAVE_EVERY = 10               # epochs between saving checkpoints and sample grids\n",
    "USE_AMP = True                # mixed precision training (recommended for speed on GPU)\n",
    "\n",
    "print(\"Hyperparameters:\")\n",
    "print(f\"EPOCHS={EPOCHS}, BATCH_SIZE={BATCH_SIZE}, LR={LR}, VAE_ZDIM={VAE_ZDIM}, VQ_EMBED_DIM={VQ_EMBED_DIM}, VQ_N_EMBED={VQ_N_EMBED}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af48170f-1cfc-4172-a8c0-fd9b683a5718",
   "metadata": {},
   "source": [
    "## üß† Dataset Preparation ‚Äî CIFAR-10\n",
    "\n",
    "This section sets up the **CIFAR-10 dataset**, which serves as the benchmark for both the **VAE** and **VQ-VAE** models.  \n",
    "CIFAR-10 is a widely used computer vision dataset containing **60,000 color images** of size **32√ó32** across **10 object categories** (e.g., airplane, car, dog, etc.).  \n",
    "It is divided into **50,000 training** and **10,000 test** images.\n",
    "\n",
    "### üîß Transformations\n",
    "The preprocessing pipeline is intentionally minimal:\n",
    "- `transforms.ToTensor()` converts each image from a PIL format into a PyTorch tensor and rescales pixel values from `[0, 255]` to `[0, 1]`.\n",
    "\n",
    "> üß© *No normalization or augmentation is applied here* ‚Äî since the goal is to analyze and compare latent representations, not to maximize classification accuracy.\n",
    "\n",
    "### ‚öôÔ∏è DataLoader Configuration\n",
    "- **`train_loader`** and **`test_loader`** efficiently stream data batches to the GPU.  \n",
    "- `batch_size` matches the previously defined `BATCH_SIZE` hyperparameter.  \n",
    "- `shuffle=True` ensures randomness in the training order for better generalization.  \n",
    "- `num_workers=4` enables multi-threaded data loading for speed.  \n",
    "- `pin_memory=True` improves GPU data transfer efficiency.\n",
    "\n",
    "Overall, this setup ensures a **lightweight, stable, and reproducible data pipeline** suitable for unsupervised generative model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5300ec8f-7605-4bc8-a65c-8ec9fd7198b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # [0,1]\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\n",
    "testset  = torchvision.datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(testset,  batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f897e678-2ba8-4968-bffe-782f25e496df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper: show grid inline\n",
    "def show_grid(tensor_imgs, nrow=8, title=None, figsize=(10,4), savepath=None):\n",
    "    grid = make_grid(tensor_imgs.cpu(), nrow=nrow, padding=2)\n",
    "    npimg = grid.numpy().transpose(1,2,0)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.axis('off')\n",
    "    if title: plt.title(title)\n",
    "    plt.imshow(np.clip(npimg, 0, 1))\n",
    "    if savepath:\n",
    "        plt.savefig(savepath, bbox_inches='tight', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "# show some real images\n",
    "batch_vis = next(iter(train_loader))[0][:16]\n",
    "show_grid(batch_vis, nrow=8, title=\"Sample real CIFAR-10 images\", figsize=(8,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef827f8-66b8-44a0-a783-e689fafe2556",
   "metadata": {},
   "source": [
    "# Convolutional Variational Autoencoder (VAE) for Image Generation\n",
    "\n",
    "Variational Autoencoders (VAEs) are a class of **probabilistic generative models** that learn to represent complex data distributions in a structured latent space. Unlike standard autoencoders, VAEs impose a **probabilistic structure** on the latent space, allowing for smooth interpolation, sample generation, and principled uncertainty estimation.\n",
    "\n",
    "This notebook presents a **convolutional VAE (ConvVAE)** architecture specifically designed for small RGB images, such as those in the CIFAR-10 dataset. By leveraging convolutional layers in both the encoder and decoder, the model can efficiently capture hierarchical spatial features, producing high-quality reconstructions while maintaining a well-regularized latent space.\n",
    "\n",
    "The key objectives of this notebook are:\n",
    "\n",
    "1. **Implementation:** Build a convolutional VAE from scratch using PyTorch.\n",
    "2. **Latent Representation:** Learn a compact, structured latent space for image data.\n",
    "3. **Image Reconstruction:** Demonstrate the model's ability to reconstruct input images with fidelity.\n",
    "4. **Generative Capability:** Sample from the latent space to generate novel images.\n",
    "5. **Analysis:** Examine reconstruction quality and latent space properties for interpretability.\n",
    "\n",
    "This work serves as both a **teaching resource** for understanding VAEs and a **foundation for research applications** in generative modeling, representation learning, and image synthesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599d92d0-84b0-4513-86d8-bfca84c59a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Standard Conv VAE implementation\n",
    "class ConvEncoderVAE(nn.Module):\n",
    "    def __init__(self, z_dim=128):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1),   # 64 x 16 x 16\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1), # 128 x 8 x 8\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1),# 256 x 4 x 4\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(256*4*4, z_dim)\n",
    "        self.fc_logvar = nn.Linear(256*4*4, z_dim)\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        h = h.view(h.size(0), -1)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "class ConvDecoderVAE(nn.Module):\n",
    "    def __init__(self, z_dim=128):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(z_dim, 256*4*4)\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1), # 128 x 8 x 8\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1),  # 64 x 16 x 16\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1),    # 3 x 32 x 32\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "    def forward(self, z):\n",
    "        h = self.fc(z).view(-1, 256, 4, 4)\n",
    "        xrec = self.deconv(h)\n",
    "        return xrec\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, z_dim=128):\n",
    "        super().__init__()\n",
    "        self.enc = ConvEncoderVAE(z_dim=z_dim)\n",
    "        self.dec = ConvDecoderVAE(z_dim=z_dim)\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.enc(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        xr = self.dec(z)\n",
    "        return xr, mu, logvar\n",
    "\n",
    "def vae_loss(recon_x, x, mu, logvar, beta=1.0):\n",
    "    # Use MSE reconstruction (works fine for CIFAR)\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "    kld = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kld, recon_loss, kld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceee5e4d-b74d-45b6-a66f-917a9935e087",
   "metadata": {},
   "source": [
    "# Vector-Quantized Variational Autoencoder (VQ-VAE)\n",
    "\n",
    "This notebook implements a **Vector-Quantized Variational Autoencoder (VQ-VAE)**, a discrete latent generative model that combines convolutional neural networks with vector quantization. Unlike standard VAEs, VQ-VAEs learn a **discrete latent codebook**, enabling efficient compression, high-quality reconstructions, and stable training. \n",
    "\n",
    "The model architecture consists of three main components:\n",
    "\n",
    "1. **Encoder:** Maps the input image to a continuous latent representation.\n",
    "2. **Vector Quantizer (VQ) with EMA):** Discretizes the latent features using a learned codebook and maintains the embeddings with **Exponential Moving Average (EMA)** updates for numerical stability.\n",
    "3. **Decoder:** Reconstructs images from quantized latent codes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf3107-c6c9-496d-9f5e-2a250cc75e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQEmbeddingEMA(nn.Module):\n",
    "    \"\"\"\n",
    "    Vector Quantizer with Exponential Moving Average (EMA) updates.\n",
    "    Ensures numerical stability and prevents NaN loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25, decay=0.99, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "        self.decay = decay\n",
    "        self.eps = eps\n",
    "\n",
    "        # Codebook initialization\n",
    "        embed = torch.randn(num_embeddings, embedding_dim)\n",
    "        self.register_buffer('embedding', embed)\n",
    "        self.register_buffer('ema_cluster_size', torch.zeros(num_embeddings))\n",
    "        self.register_buffer('ema_w', embed.clone())\n",
    "        self.initialized = False\n",
    "\n",
    "    def _initialize_embeddings(self, flat):\n",
    "        \"\"\"Initialize embeddings from first batch.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            N = flat.size(0)\n",
    "            n = min(self.num_embeddings, N)\n",
    "            perm = torch.randperm(N, device=flat.device)\n",
    "            init = flat[perm[:n]]\n",
    "            if n < self.num_embeddings:\n",
    "                pad = torch.randn(self.num_embeddings - n, flat.size(1), device=flat.device)\n",
    "                init = torch.cat([init, pad], dim=0)\n",
    "            self.embedding.copy_(init)\n",
    "            self.ema_w.copy_(init)\n",
    "            self.ema_cluster_size.zero_()\n",
    "            self.initialized = True\n",
    "\n",
    "    def forward(self, z_e):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z_e: Encoder output (B, C, H, W)\n",
    "        Returns:\n",
    "            quantized: Discretized latent\n",
    "            vq_loss: Quantization + commitment loss\n",
    "            indices: Code indices\n",
    "        \"\"\"\n",
    "        b, c, h, w = z_e.shape\n",
    "        device = z_e.device\n",
    "        flat = z_e.permute(0, 2, 3, 1).contiguous().view(-1, c)\n",
    "\n",
    "        # Lazy initialization\n",
    "        if not self.initialized:\n",
    "            self._initialize_embeddings(flat)\n",
    "\n",
    "        # Compute L2 distances\n",
    "        distances = (\n",
    "            torch.sum(flat ** 2, dim=1, keepdim=True)\n",
    "            - 2 * flat @ self.embedding.t()\n",
    "            + torch.sum(self.embedding ** 2, dim=1).unsqueeze(0)\n",
    "        )\n",
    "\n",
    "        encoding_indices = torch.argmin(distances, dim=1)\n",
    "        encodings = F.one_hot(encoding_indices, self.num_embeddings).type(flat.dtype)\n",
    "\n",
    "        # Quantized output\n",
    "        quantized = (encodings @ self.embedding).view(b, h, w, c).permute(0, 3, 1, 2)\n",
    "\n",
    "        # EMA updates (only during training)\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                enc_sum = encodings.sum(0)\n",
    "                dw = encodings.t() @ flat\n",
    "\n",
    "                self.ema_cluster_size.mul_(self.decay).add_(enc_sum, alpha=1 - self.decay)\n",
    "                self.ema_w.mul_(self.decay).add_(dw, alpha=1 - self.decay)\n",
    "\n",
    "                # Normalize embeddings\n",
    "                n = self.ema_cluster_size.sum()\n",
    "                cluster_size = ((self.ema_cluster_size + self.eps) /\n",
    "                                (n + self.num_embeddings * self.eps)) * n\n",
    "                self.embedding.copy_(self.ema_w / cluster_size.unsqueeze(1))\n",
    "\n",
    "        # Compute VQ loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), z_e)\n",
    "        q_latent_loss = F.mse_loss(quantized, z_e.detach())\n",
    "        vq_loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "\n",
    "        # Straight-through estimator\n",
    "        quantized = z_e + (quantized - z_e).detach()\n",
    "        indices = encoding_indices.view(b, h, w)\n",
    "\n",
    "        return quantized, vq_loss, indices\n",
    "\n",
    "# -----------------------------\n",
    "# VQ-VAE model\n",
    "# -----------------------------\n",
    "class VQVAE(nn.Module):\n",
    "    def __init__(self, embedding_dim=64, n_embeddings=512, encoder_base_channels=(128,256)):\n",
    "        super().__init__()\n",
    "        ch1, ch2 = encoder_base_channels\n",
    "\n",
    "        # Encoder network\n",
    "        self.enc_body = nn.Sequential(\n",
    "            nn.Conv2d(3, ch1, 4, 2, 1),  # 32 ‚Üí 16\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(ch1, ch2, 4, 2, 1),  # 16 ‚Üí 8\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "        # Project to embedding dimension\n",
    "        self.enc_to_vq = nn.Conv2d(ch2, embedding_dim, 1)\n",
    "\n",
    "        # Vector Quantizer\n",
    "        self.quantizer = VQEmbeddingEMA(n_embeddings, embedding_dim, commitment_cost=0.25)\n",
    "\n",
    "        # Decoder network\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(embedding_dim, 256, 4, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 3, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def enc(self, x):\n",
    "        \"\"\"Encodes input to latent feature map before quantization.\"\"\"\n",
    "        return self.enc_to_vq(self.enc_body(x))\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.enc_body(x)\n",
    "        z_e = self.enc_to_vq(z)\n",
    "        z_q, vq_loss, indices = self.quantizer(z_e)\n",
    "        x_recon = self.dec(z_q)\n",
    "        return x_recon, vq_loss, indices\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81289ad4-8b1e-4660-a07e-f84a0fb8c95c",
   "metadata": {},
   "source": [
    "# Generative Modeling with VAE and VQ-VAE\n",
    "\n",
    "This notebook demonstrates the implementation and training of two powerful generative models for image reconstruction and synthesis: a **Convolutional Variational Autoencoder (VAE)** and a **Vector-Quantized VAE (VQ-VAE)**. Both models leverage convolutional neural networks to capture hierarchical spatial features in images, while employing distinct latent representations.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Convolutional Variational Autoencoder (VAE)\n",
    "\n",
    "The VAE is a probabilistic generative model that maps input images to a **continuous latent space** and reconstructs them using a decoder network.\n",
    "\n",
    "- **Encoder:** Convolutional layers reduce spatial resolution and output the **mean (`Œº`)** and **log-variance (`logœÉ¬≤`)** of a latent Gaussian distribution.\n",
    "- **Reparameterization Trick:** Samples latent vector \\( z = \\mu + \\sigma \\cdot \\epsilon \\) to allow gradient backpropagation through stochastic nodes.\n",
    "- **Decoder:** Transpose convolutions reconstruct the image from latent samples.\n",
    "- **Loss Function:** Combines reconstruction loss (Mean Squared Error) and Kullback-Leibler divergence:\n",
    "\\[\n",
    "\\mathcal{L}_{VAE} = \\text{MSE}(\\hat{x}, x) + \\beta \\cdot \\text{KL}(q(z|x)\\|p(z))\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Vector-Quantized VAE (VQ-VAE)\n",
    "\n",
    "The VQ-VAE uses a **discrete latent space** with a learned codebook of embeddings, enabling high-quality image reconstruction and efficient compression.\n",
    "\n",
    "- **Encoder:** Maps images to a latent feature map.\n",
    "- **Vector Quantizer with EMA:** Discretizes latent features by assigning each feature to its nearest codebook vector. Exponential Moving Average (EMA) updates maintain numerical stability and prevent NaN losses.\n",
    "- **Decoder:** Reconstructs images from quantized latent codes using transpose convolutions.\n",
    "- **Loss Function:** Combines reconstruction loss and commitment cost to enforce latent alignment:\n",
    "\\[\n",
    "\\mathcal{L}_{VQ} = \\text{MSE}(z_q, z_e.\\text{detach}()) + \\beta \\cdot \\text{MSE}(z_q.\\text{detach}(), z_e)\n",
    "\\]\n",
    "- **Straight-Through Estimator:** Ensures gradients flow through the non-differentiable quantization operation.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Training Setup\n",
    "\n",
    "- **Optimizers:** `Adam` is used separately for VAE and VQ-VAE.\n",
    "- **Mixed-Precision Training:** Optional AMP via `GradScaler` for faster computation and lower memory usage.\n",
    "- **Metric Tracking:** History dictionaries record reconstruction loss, KL divergence, and quantization losses per epoch.\n",
    "- **Visualization Utility:** `save_grid()` function saves reconstructed or generated images in a grid format for inspection.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Summary\n",
    "\n",
    "This notebook establishes a **complete generative modeling pipeline**:\n",
    "\n",
    "1. **VAE**: Continuous latent space for smooth interpolation and probabilistic reconstruction.\n",
    "2. **VQ-VAE**: Discrete latent space with EMA-updated codebook for high-fidelity image generation.\n",
    "3. **Optimized Training**: Separate optimizers and optional AMP scalers for efficiency.\n",
    "4. **Evaluation**: Metric tracking and image grid visualization enable monitoring of reconstruction quality and training progress.\n",
    "\n",
    "Together, these components provide a robust framework for **image representation learning, generative modeling, and latent-space exploration**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8028f2-26a1-4a5f-9640-fe5a8fffa2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Instantiate models, optimizers, scalers\n",
    "vae = VAE(z_dim=VAE_ZDIM).to(device)\n",
    "vqvae = VQVAE(embedding_dim=VQ_EMBED_DIM, n_embeddings=VQ_N_EMBED).to(device)\n",
    "\n",
    "opt_vae = torch.optim.Adam(vae.parameters(), lr=LR)\n",
    "opt_vq  = torch.optim.Adam(vqvae.parameters(), lr=LR)\n",
    "\n",
    "scaler_vae = GradScaler('cuda', enabled=USE_AMP)\n",
    "scaler_vq  = GradScaler('cuda', enabled=USE_AMP)\n",
    "\n",
    "# trackers\n",
    "history = {\n",
    "    'vae_loss': [], 'vae_recon': [], 'vae_kld': [],\n",
    "    'vq_loss': [], 'vq_recon': [], 'vq_vqterm': []\n",
    "}\n",
    "\n",
    "# helper: save samples grid\n",
    "def save_grid(tensor, path, nrow=8):\n",
    "    save_image(tensor.cpu(), path, nrow=nrow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f362821-dcf6-4827-b899-87495bd3da88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training loops with visualization\n",
    "def train_epoch_vae(model, dataloader, optimizer, scaler, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_recon = 0.0\n",
    "    running_kld = 0.0\n",
    "    pbar = tqdm(dataloader, desc=f\"VAE Train E{epoch}\")\n",
    "    for x, _ in pbar:\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with autocast('cuda', enabled=USE_AMP):\n",
    "            xr, mu, logvar = model(x)\n",
    "            loss, rec, kld = vae_loss(xr, x, mu, logvar, beta=BETA)\n",
    "            loss_val = loss\n",
    "        scaler.scale(loss_val).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        running_loss += loss_val.item()\n",
    "        running_recon += rec.item()\n",
    "        running_kld += kld.item()\n",
    "        pbar.set_postfix({'loss': f\"{running_loss/((pbar.n+1)*x.size(0)):.4f}\"})\n",
    "    n = len(dataloader.dataset)\n",
    "    return running_loss/n, running_recon/n, running_kld/n\n",
    "\n",
    "def train_epoch_vqvae(model, dataloader, optimizer, scaler, epoch, device):\n",
    "    model.train()\n",
    "    total_loss, recon_loss, vq_loss = 0, 0, 0\n",
    "\n",
    "    for x, _ in tqdm(dataloader, desc=f\"VQ-VAE Train E{epoch}\"):\n",
    "        x = x.to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            x_recon, vq_loss_item, _ = model(x)\n",
    "            recon = F.mse_loss(x_recon, x)\n",
    "            loss = recon + vq_loss_item\n",
    "\n",
    "        # Skip invalid loss\n",
    "        if torch.isnan(loss):\n",
    "            continue\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        recon_loss += recon.item()\n",
    "        vq_loss += vq_loss_item.item()\n",
    "\n",
    "    n = len(dataloader)\n",
    "    return total_loss/n, recon_loss/n, vq_loss/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b9d39f-ef77-45ec-a84c-bc746b5741dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small helper to show reconstructions\n",
    "@torch.no_grad()\n",
    "def visualize_reconstructions(model, mode='vae', n=8, savepath=None, step_label=\"\"):\n",
    "    model.eval()\n",
    "    x = next(iter(test_loader))[0][:n].to(device)\n",
    "    if mode == 'vae':\n",
    "        xr, mu, logvar = model(x)\n",
    "    else:  # vqvae\n",
    "        xr, vq_loss, _ = model(x)\n",
    "    combined = torch.cat([x.cpu(), xr.cpu()], dim=0)\n",
    "    title = f\"{mode.upper()} Reconstructions {step_label}\"\n",
    "    if savepath:\n",
    "        save_grid(combined, savepath, nrow=n)\n",
    "    show_grid(combined, nrow=n, title=title)\n",
    "\n",
    "# collect code-index histogram (for sampling later)\n",
    "@torch.no_grad()\n",
    "def collect_vq_code_histogram(vq_model, loader):\n",
    "    vq_model.eval()\n",
    "    counts = torch.zeros(vq_model.quantizer.num_embeddings, device='cpu')\n",
    "    for x, _ in tqdm(loader, desc=\"Collect VQ code histogram\"):\n",
    "        x = x.to(device)\n",
    "        _, _, indices = vq_model(x)\n",
    "        indices_cpu = indices.cpu().view(-1)\n",
    "        for idx in indices_cpu:\n",
    "            counts[idx] += 1\n",
    "    probs = (counts + 1e-6) / (counts.sum() + 1e-6 * counts.size(0))  # smooth\n",
    "    return probs.numpy()\n",
    "\n",
    "# sampling for VAE\n",
    "@torch.no_grad()\n",
    "def sample_vae(model, n, z_dim):\n",
    "    model.eval()\n",
    "    z = torch.randn(n, z_dim, device=device)\n",
    "    xr = model.dec(z)\n",
    "    return xr.clamp(0,1)\n",
    "\n",
    "# sampling for VQ-VAE using empirical code distribution\n",
    "@torch.no_grad()\n",
    "def sample_vqvae_empirical(vq_model, n, code_probs=None):\n",
    "    vq_model.eval()\n",
    "    dummy = torch.zeros(1, 3, 32, 32, device=device)\n",
    "\n",
    "    # use new encoder\n",
    "    ze = vq_model.enc_to_vq(vq_model.enc_body(dummy))\n",
    "    _, c, h, w = ze.shape\n",
    "\n",
    "    if code_probs is None:\n",
    "        code_probs = np.ones(vq_model.quantizer.num_embeddings) / vq_model.quantizer.num_embeddings\n",
    "\n",
    "    # sample index grid\n",
    "    idx = np.random.choice(vq_model.quantizer.num_embeddings, size=(n,h,w), p=code_probs)\n",
    "\n",
    "    # get embeddings\n",
    "    emb = vq_model.quantizer.embedding.to(device)  # [E, C]\n",
    "    emb_t = emb[idx]  # n,h,w,C\n",
    "    # convert to n,C,H,W\n",
    "    emb_t = torch.tensor(emb[idx], dtype=torch.float32, device=device).permute(0,3,1,2)\n",
    "\n",
    "    xr = vq_model.dec(emb_t)\n",
    "    return xr.clamp(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac54523-809d-41ff-b95b-bee295993cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup output directory\n",
    "OUTDIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "log_file = OUTDIR / \"training.log\"\n",
    "logging.basicConfig(\n",
    "    filename=str(log_file),\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    filemode='w'\n",
    ")\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "console.setFormatter(formatter)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "logging.info(\"üöÄ Starting training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f92ba-7a67-4720-abc3-8c7f0d625618",
   "metadata": {},
   "source": [
    "# VAE Training Loop\n",
    "\n",
    "This cell implements the **training procedure for the convolutional VAE**, including mixed-precision support, metric tracking, checkpointing, and sample visualization.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. GradScaler for Mixed-Precision Training\n",
    "\n",
    "```python\n",
    "scaler_vae = torch.amp.GradScaler(\"cuda\", enabled=USE_AMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e32dd5-5a26-4e5e-9deb-a9407268b512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- GradScaler\n",
    "scaler_vae = torch.amp.GradScaler(\"cuda\", enabled=USE_AMP)\n",
    "\n",
    "history_vae = {\"loss\": [], \"recon\": [], \"kld\": []}\n",
    "best_vaeloss = float(\"inf\")\n",
    "\n",
    "vae.train()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss_epoch, rec_epoch, kld_epoch = train_epoch_vae(\n",
    "        vae, train_loader, opt_vae, scaler_vae, epoch\n",
    "    )\n",
    "\n",
    "    history_vae[\"loss\"].append(loss_epoch)\n",
    "    history_vae[\"recon\"].append(rec_epoch)\n",
    "    history_vae[\"kld\"].append(kld_epoch)\n",
    "\n",
    "    logging.info(f\"VAE Epoch {epoch}: total={loss_epoch:.4f}, recon={rec_epoch:.4f}, kld={kld_epoch:.4f}\")\n",
    "\n",
    "    # Periodic reconstructions\n",
    "    if epoch % SAVE_EVERY == 0 or epoch == 1 or epoch == EPOCHS:\n",
    "        vpath = OUTDIR / f\"vae_recon_e{epoch}.png\"\n",
    "        visualize_reconstructions(vae, mode='vae', n=8, savepath=str(vpath), step_label=f\"epoch{epoch}\")\n",
    "\n",
    "        vae_samples = sample_vae(vae, n=min(36, NUM_SAMPLES), z_dim=VAE_ZDIM)\n",
    "        save_grid(vae_samples, OUTDIR / f\"vae_samples_e{epoch}.png\", nrow=6)\n",
    "\n",
    "    # Save checkpoint\n",
    "    if epoch % SAVE_EVERY == 0 or epoch == EPOCHS:\n",
    "        torch.save(vae.state_dict(), OUTDIR / f\"vae_epoch{epoch}.pt\")\n",
    "        logging.info(f\"üíæ VAE checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "# Final save\n",
    "torch.save(vae.state_dict(), OUTDIR / \"vae_final.pt\")\n",
    "logging.info(\"‚úÖ VAE training complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c51626-4d7b-4cd9-9e2a-3b50e1924aab",
   "metadata": {},
   "source": [
    "# VQ-VAE Training Loop\n",
    "\n",
    "This cell implements the **training procedure for the Vector-Quantized Variational Autoencoder (VQ-VAE)**, including mixed-precision support, metric tracking, checkpointing, and sample visualization.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. GradScaler for Mixed-Precision Training\n",
    "\n",
    "```python\n",
    "scaler_vq = torch.amp.GradScaler(enabled=USE_AMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48683adf-dfdb-49f8-91e4-474f8e13ff17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# GradScaler\n",
    "# ------------------------\n",
    "scaler_vq = torch.amp.GradScaler(enabled=USE_AMP)\n",
    "\n",
    "# History tracking\n",
    "history_vq = {\"loss\": [], \"recon\": [], \"vqterm\": []}\n",
    "best_vqloss = float(\"inf\")\n",
    "\n",
    "# Ensure output folder exists\n",
    "(VQDIR := OUTDIR / \"vq_vae\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------\n",
    "# Training loop\n",
    "# ------------------------\n",
    "vqvae.train()\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss_epoch, rec_epoch, vq_epoch = train_epoch_vqvae(\n",
    "        vqvae, train_loader, opt_vq, scaler_vq, epoch, device\n",
    "    )\n",
    "\n",
    "    history_vq[\"loss\"].append(loss_epoch)\n",
    "    history_vq[\"recon\"].append(rec_epoch)\n",
    "    history_vq[\"vqterm\"].append(vq_epoch)\n",
    "\n",
    "    logging.info(\n",
    "        f\"VQ-VAE Epoch {epoch}: total={loss_epoch:.4f}, recon={rec_epoch:.4f}, vq={vq_epoch:.4f}\"\n",
    "    )\n",
    "\n",
    "    # ------------------------\n",
    "    # Periodic reconstructions\n",
    "    # ------------------------\n",
    "    if epoch % SAVE_EVERY == 0 or epoch == 1 or epoch == EPOCHS:\n",
    "        vqpath = VQDIR / f\"vq_recon_e{epoch}.png\"\n",
    "        visualize_reconstructions(\n",
    "            vqvae, mode='vq', n=8, savepath=str(vqpath), step_label=f\"epoch{epoch}\"\n",
    "        )\n",
    "\n",
    "        # Fixed empirical sampling\n",
    "        with torch.no_grad():\n",
    "            vqvae.eval()\n",
    "            dummy = torch.zeros(1, 3, 32, 32, device=device)\n",
    "            ze = vqvae.enc_to_vq(vqvae.enc_body(dummy))\n",
    "            _, C, H, W = ze.shape\n",
    "\n",
    "            # sample indices uniformly or with given probabilities\n",
    "            idx = torch.randint(\n",
    "                0, vqvae.quantizer.num_embeddings, (min(36, NUM_SAMPLES), H, W), device=device\n",
    "            )\n",
    "            emb = vqvae.quantizer.embedding.to(device)  # [E, C]\n",
    "            emb_t = emb[idx].permute(0,3,1,2)           # n, C, H, W\n",
    "            xr = vqvae.dec(emb_t).clamp(0,1)\n",
    "\n",
    "        save_image(xr, VQDIR / f\"vq_samples_e{epoch}.png\", nrow=6)\n",
    "\n",
    "    # ------------------------\n",
    "    # Save checkpoint\n",
    "    # ------------------------\n",
    "    if epoch % SAVE_EVERY == 0 or epoch == EPOCHS:\n",
    "        torch.save(vqvae.state_dict(), VQDIR / f\"vqvae_epoch{epoch}.pt\")\n",
    "        logging.info(f\"üíæ VQ-VAE checkpoint saved at epoch {epoch}\")\n",
    "\n",
    "# ------------------------\n",
    "# Final save\n",
    "# ------------------------\n",
    "torch.save(vqvae.state_dict(), VQDIR / \"vqvae_final.pt\")\n",
    "logging.info(\"‚úÖ VQ-VAE training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d7f5b2-ffb9-4a17-8768-6a4aa82cc76b",
   "metadata": {},
   "source": [
    "# Collecting VQ-VAE Code Histogram\n",
    "\n",
    "This cell computes and visualizes the **empirical distribution of VQ-VAE codebook assignments** across the training dataset. Understanding this distribution is critical for **improving sample quality** when generating images from the discrete latent space.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Motivation\n",
    "\n",
    "- The **VQ-VAE latent space** consists of a fixed set of discrete embeddings (codebook vectors).  \n",
    "- During training, each latent feature is assigned to its **nearest codebook vector**.\n",
    "- Some embeddings may be underrepresented or never used, leading to **poor sampling quality** if codebook indices are sampled uniformly.  \n",
    "- By collecting a **histogram of code assignments**, we can:\n",
    "  1. Identify rarely-used codes.\n",
    "  2. Compute **empirical probabilities** for more realistic latent sampling.\n",
    "  3. Improve the fidelity of generated images.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Code Histogram Collection\n",
    "\n",
    "```python\n",
    "print(\"Collecting VQ code histogram from training set (this helps VQ-VAE sampling quality)...\")\n",
    "code_probs = collect_vq_code_histogram(vqvae, train_loader)  # numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0563446-d9ce-40d6-8b50-8f71ec843560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Collect VQ code histogram (for better sampling)\n",
    "print(\"Collecting VQ code histogram from training set (this helps VQ-VAE sampling quality)...\")\n",
    "code_probs = collect_vq_code_histogram(vqvae, train_loader)  # numpy array\n",
    "# save histogram plot\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.bar(np.arange(len(code_probs)), code_probs, width=1.0)\n",
    "plt.title(\"VQ code empirical distribution (first 200 shown)\")\n",
    "plt.xlim(0, min(200, len(code_probs)))\n",
    "plt.savefig(OUTDIR / \"vq_code_histogram.png\", dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538aeb81-d266-4c6b-95bf-f9d6c403668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Sampling and Comparison: VAE vs VQ-VAE\n",
    "\n",
    "This cell generates **final samples** from both the trained **VAE** and **VQ-VAE** models and visualizes them side-by-side for qualitative comparison.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Sample Generation\n",
    "\n",
    "```python\n",
    "n = NUM_SAMPLES\n",
    "vae_samples = sample_vae(vae, n=n, z_dim=VAE_ZDIM)\n",
    "vq_samples  = sample_vqvae_empirical(vqvae, n=n, code_probs=code_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfd75c7-cb5d-4063-8c7f-1c65a5070a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = NUM_SAMPLES\n",
    "print(f\"Generating {n} samples from each model...\")\n",
    "\n",
    "vae_samples = sample_vae(vae, n=n, z_dim=VAE_ZDIM)\n",
    "vq_samples  = sample_vqvae_empirical(vqvae, n=n, code_probs=code_probs)\n",
    "\n",
    "# Save grids separately\n",
    "save_grid(vae_samples, OUTDIR / f\"vae_samples_final.png\", nrow=min(10,n))\n",
    "save_grid(vq_samples,  OUTDIR / f\"vq_samples_final.png\",  nrow=min(10,n))\n",
    "\n",
    "# Combine both in a single figure: first row VAE, second row VQ-VAE\n",
    "combined = torch.cat([vae_samples, vq_samples], dim=0)\n",
    "show_grid(combined, nrow=min(10,n), title=f\"Top: VAE ({n} samples). Bottom: VQ-VAE ({n} samples)\", figsize=(16,6),\n",
    "          savepath=OUTDIR / \"comparison_final.png\")\n",
    "\n",
    "print(\"Saved final sample grids to\", OUTDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3877c390-f097-4dc7-b540-467593528e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Summary printout and where outputs are saved\n",
    "print(\"All done. Outputs saved in:\", OUTDIR)\n",
    "print(\"- Model checkpoints: vae_final.pt, vqvae_final.pt (and epoch checkpoints)\")\n",
    "print(\"- Reconstructions: vae_recon_*.png, vq_recon_*.png\")\n",
    "print(\"- Sample grids: vae_samples_final.png, vq_samples_final.png, comparison_final.png\")\n",
    "print(\"- Training loss plot: train_loss_curves.png\")\n",
    "print(\"- VQ code histogram: vq_code_histogram.png\")\n",
    "print(\"- Interpolation/morph visuals: vae_interpolation.png, vq_morph.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
